# Learning Rate
lr_encoder: 1e-5
lr_others: 5e-5

# Training Parameters
num_steps: 60000
warmup_ratio: 0.1
train_batch_size: 4
eval_every: 10000

# Model Configuration
max_width: 12
model_name: microsoft/deberta-v3-large # hugging face model
fine_tune: true
subtoken_pooling: first
hidden_size: 768
span_mode: markerV0
dropout: 0.4

# "none" if no pretrained model 
prev_path: "none"


# Training Specifics
size_sup: -1
max_types: 25
shuffle_types: true
random_drop: true
max_neg_type_ratio: 1
max_len: 384

name: "large"


